{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Linear Algebra (Continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6: Special Kinds of Matrices\n",
    "\n",
    "### Diagonal Matrix\n",
    "\n",
    "A matrix is __diagonal__ if it contains zero in all elements except along the main diagonal. I.e. all position except where $i = j$ are 0.  Note that a diagonal matrix does not need to be square."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.8 0.  0.  0.  0. ]\n",
      " [0.  1.8 0.  0.  0. ]\n",
      " [0.  0.  1.8 0.  0. ]\n",
      " [0.  0.  0.  1.8 0. ]\n",
      " [0.  0.  0.  0.  1.8]]\n"
     ]
    }
   ],
   "source": [
    "# Creating a diagonal matrix in numPy:\n",
    "\n",
    "a = np.zeros((5, 5), np.float16)\n",
    "np.fill_diagonal(a, 1.8)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  4  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  3  0  0  0  0  0  0  0]\n",
      " [ 0  0  0 55  0  0  0  0  0  0]\n",
      " [ 0  0  0  0 41  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 -4  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# Another example\n",
    "a = np.zeros((6, 10), np.int)\n",
    "\n",
    "np.fill_diagonal(a, [-2, 4, 3, 55, 41, -4])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -2   0   0   0   0   0   0   0]\n",
      " [  0   4   0   0   0   0   0   0]\n",
      " [  0   0   3   0   0   0   0   0]\n",
      " [  0   0   0  55   0   0   0   0]\n",
      " [  0   0   0   0   3   0   0   0]\n",
      " [  0   0   0   0   0   4   0   0]\n",
      " [  0   0   0   0   0   0   7   0]\n",
      " [  0   0   0   0   0   0   0 -12]]\n"
     ]
    }
   ],
   "source": [
    "# Another example using np.diag()\n",
    "\n",
    "print(np.diag([-2, 4, 3, 55, 3, 4, 7, -12]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonal matrices are of interest because they are computationally efficient when used in multiplication.\n",
    "\n",
    "$diag(v)$ denotes a square diagonal matrix whose diagonal is given by the vector $v$ - note that this corresponds to the numPy function name.\n",
    "\n",
    "E.g. given $diag(v)x$, the result is just each element of $x_i$ by $v_i$. We can use numPy as an example of this special case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A square diagonal matrix diag([1, 2, 3, 4]):\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 2. 0. 0.]\n",
      " [0. 0. 3. 0.]\n",
      " [0. 0. 0. 4.]]\n",
      "---\n",
      "[[1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "---\n",
      "[[ 1.]\n",
      " [ 4.]\n",
      " [ 9.]\n",
      " [16.]]\n"
     ]
    }
   ],
   "source": [
    "# To demonstrate the equivalence, we can set up an example using numPy:\n",
    "\n",
    "print(\"A square diagonal matrix diag([1, 2, 3, 4]):\")\n",
    "a = np.zeros((4, 4), np.float16)\n",
    "np.fill_diagonal(a, [1, 2, 3, 4])\n",
    "print(a)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "b = np.array([[1], [2], [3], [4]])\n",
    "print(b)\n",
    "\n",
    "print(\"---\")\n",
    "c_2 = np.dot(a, b)\n",
    "print(c_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also very easy to compute the inverse of a square diagonal matrix.\n",
    "\n",
    "If a square diagonal has all-nonzero diagonal elements, the inverse is the diagonal matrix with one over each of the elements in the original; i.e.:\n",
    "\n",
    "$$ \\large diag(v)^{−1} = diag([1/v_1, \\ldots ,1/v_n]^\\top) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A square diagonal matrix diag([1, 2, 2, 10, 8]):\n",
      "[[ 1.  0.  0.  0.  0.]\n",
      " [ 0.  2.  0.  0.  0.]\n",
      " [ 0.  0.  2.  0.  0.]\n",
      " [ 0.  0.  0. 10.  0.]\n",
      " [ 0.  0.  0.  0.  8.]]\n",
      "---\n",
      "A square diagonal matrix diag([1/1, 1/2, 1/2, 1/6, 1/8]):\n",
      "[[1.    0.    0.    0.    0.   ]\n",
      " [0.    0.5   0.    0.    0.   ]\n",
      " [0.    0.    0.5   0.    0.   ]\n",
      " [0.    0.    0.    0.1   0.   ]\n",
      " [0.    0.    0.    0.    0.125]]\n",
      "---\n",
      "Multiplying these gives the identity matrix):\n",
      "[[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Demonstrating this property using numPy:\n",
    "\n",
    "print(\"A square diagonal matrix diag([1, 2, 2, 10, 8]):\")\n",
    "a = np.zeros((5, 5), np.float16)\n",
    "np.fill_diagonal(a, [1, 2, 2, 10, 8])\n",
    "print(a)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "print(\"A square diagonal matrix diag([1/1, 1/2, 1/2, 1/6, 1/8]):\")\n",
    "b = np.zeros((5, 5), np.float16)\n",
    "np.fill_diagonal(b, [1/1, 1/2, 1/2, 1/10, 1/8])\n",
    "print(b)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "print(\"Multiplying these gives the identity matrix):\")\n",
    "c = a.dot(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse does not exist for a diagonal matrix if any of the elements along the diagonal are zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Matrix\n",
    "\n",
    "A __symmetric matrix__ is equal to its own transpose (and is relevant only for square matrices); i.e.:\n",
    "\n",
    "$$ \\large A = A^{\\top} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  16   3]\n",
      " [ 16  -8   1]\n",
      " [  3   1 -42]]\n",
      "---\n",
      "[[  2  16   3]\n",
      " [ 16  -8   1]\n",
      " [  3   1 -42]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[2, 16, 3], [16, -8, 1], [3, 1, -42]])\n",
    "a_transpose = a.T\n",
    "\n",
    "print(a)\n",
    "print(\"---\")\n",
    "print(a_transpose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Vector\n",
    "\n",
    "__A unit vector__ has a length of 1.  In other words, it is a vector with __unit norm__.\n",
    "\n",
    "$$ \\Large \\left\\Vert x\\right\\Vert_2 = 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Vectors\n",
    "\n",
    "__Orthogonal vectors__ have a 90-degree angle relative to each other, and are vectors $x$ and $y$ where $x^\\top y = 0$.\n",
    "\n",
    "If orthogonal vectors are __orthogonal__ and have __unit norm__, they are said to be __orthonormal__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orthogonal Matrices\n",
    "\n",
    "An orthogonal matrix is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal. The inverse of an orthogonal matrix is cheap to compute due to its structure.\n",
    "\n",
    "$$ \\Large AA = AA^{\\top} = I $$\n",
    "\n",
    "This implies: \n",
    "\n",
    "$$ \\Large A^{−1} = A^{\\top} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.50510599  0.26864664  0.72235747  0.38845413]\n",
      " [ 0.16286063  0.74461036 -0.42541806  0.48790502]\n",
      " [-0.64013477  0.38747638 -0.26456844 -0.60835276]\n",
      " [ 0.55549243  0.47248964  0.47667883 -0.49087575]]\n",
      "---\n",
      "[[ 1.00000000e+00 -2.44422054e-16 -1.75249628e-16  2.95552363e-16]\n",
      " [-2.44422054e-16  1.00000000e+00  2.74904615e-16  1.67722865e-16]\n",
      " [-1.75249628e-16  2.74904615e-16  1.00000000e+00 -7.39982285e-17]\n",
      " [ 2.95552363e-16  1.67722865e-16 -7.39982285e-17  1.00000000e+00]]\n",
      "---\n",
      "[[-0.50510599  0.16286063 -0.64013477  0.55549243]\n",
      " [ 0.26864664  0.74461036  0.38747638  0.47248964]\n",
      " [ 0.72235747 -0.42541806 -0.26456844  0.47667883]\n",
      " [ 0.38845413  0.48790502 -0.60835276 -0.49087575]]\n",
      "[[-0.50510599  0.16286063 -0.64013477  0.55549243]\n",
      " [ 0.26864664  0.74461036  0.38747638  0.47248964]\n",
      " [ 0.72235747 -0.42541806 -0.26456844  0.47667883]\n",
      " [ 0.38845413  0.48790502 -0.60835276 -0.49087575]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import ortho_group\n",
    "\n",
    "x = ortho_group.rvs(4)\n",
    "\n",
    "print(x)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "b = np.dot(x, x.T)\n",
    "\n",
    "print(b)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "x_inv = np.linalg.inv(x)\n",
    "\n",
    "x_T = x.T\n",
    "\n",
    "print(x_inv)\n",
    "\n",
    "print(x_T)\n",
    "\n",
    "np.allclose(x_inv, x_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigendecomposition\n",
    "\n",
    "Eigendecomposition of matrices is the factorizing of matrices into its canonical form - it's representation as the product of its __eigenvalues__ and __eigenvectors__.\n",
    "\n",
    "It's very useful to think of matrices as __linear transformations__.  We can think of \"applying\" matrices to a vector, resulting in some linear transformation of space - i.e. rotation and scaling, resulting in an output vector transformed in this way.  Applying a matrix is a way of saying we compute the dot product of the matrix with the initial vector to produce the output vector.\n",
    "\n",
    "> An __eigenvector__ of of a square matrix $A$ is a nonzero vector $v$ such that multiplication by $A$ alters only the _scale_ of $v$.\n",
    "\n",
    "$$ \\Large Av = \\lambda v $$\n",
    "\n",
    "$\\lambda$ is known as the __eigenvalue__ corresponding to this eigenvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.00147145, 0.94827066]),\n",
       " array([[-0.99652227, -0.70442649],\n",
       "        [ 0.08332685, -0.70977695]]))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can calculate the eigenvalues and right eigenvector of a matrix using numPy:\n",
    "\n",
    "A = np.random.rand(2,2)\n",
    "\n",
    "np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
